{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0M9nxD5aanZ",
        "outputId": "000aa750-64c1-4332-d32f-8dd59c4299ac"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HMxs07UsdZJR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tar: /Downloads/hi_fi_tts_v0.tar.gz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "zsh:cd:1: no such file or directory: /datasets/hi_fi_tts_v0/\n"
          ]
        }
      ],
      "source": [
        "! tar -xvf '/Downloads/hi_fi_tts_v0.tar.gz' -C '/datasets/hi_fi_tts_v0/'\n",
        "! cd '/datasets/hi_fi_tts_v0/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6skloSKnZklL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: audiofile in /home/server2/.local/lib/python3.10/site-packages (1.3.0)\n",
            "Requirement already satisfied: audeer in /home/server2/.local/lib/python3.10/site-packages (from audiofile) (1.20.1)\n",
            "Requirement already satisfied: audmath>=1.2.1 in /home/server2/.local/lib/python3.10/site-packages (from audiofile) (1.3.0)\n",
            "Requirement already satisfied: numpy in /home/server2/.local/lib/python3.10/site-packages (from audiofile) (1.22.0)\n",
            "Requirement already satisfied: soundfile in /home/server2/.local/lib/python3.10/site-packages (from audiofile) (0.12.1)\n",
            "Requirement already satisfied: tqdm in /home/server2/.local/lib/python3.10/site-packages (from audeer->audiofile) (4.65.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /home/server2/.local/lib/python3.10/site-packages (from soundfile->audiofile) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /home/server2/.local/lib/python3.10/site-packages (from cffi>=1.0->soundfile->audiofile) (2.21)\n",
            "\u001b[33mDEPRECATION: torchsde 0.2.5 has a non-standard dependency specifier numpy>=1.19.*; python_version >= \"3.7\". pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of torchsde or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: voicefixer 0.1.2 has a non-standard dependency specifier streamlit>=1.12.0pyyaml. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of voicefixer or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: distro-info 1.1build1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: python-debian 0.1.43ubuntu1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of python-debian or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pydub in /home/server2/.local/lib/python3.10/site-packages (0.25.1)\n",
            "\u001b[33mDEPRECATION: torchsde 0.2.5 has a non-standard dependency specifier numpy>=1.19.*; python_version >= \"3.7\". pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of torchsde or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: voicefixer 0.1.2 has a non-standard dependency specifier streamlit>=1.12.0pyyaml. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of voicefixer or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: distro-info 1.1build1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: python-debian 0.1.43ubuntu1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of python-debian or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install audiofile\n",
        "! pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3mRMedDVVgxA"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import audiofile\n",
        "from pydub import AudioSegment\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "36uIHMItYSJ0"
      },
      "outputs": [],
      "source": [
        "import torchaudio\n",
        "# coverts flac files to wavs and removes flac\n",
        "\n",
        "def convert_path(audio_dir_path, target_suffix):\n",
        "  if not audio_dir_path.is_dir():\n",
        "    if audio_dir_path.suffix == target_suffix:\n",
        "      return [audio_dir_path]\n",
        "    return []\n",
        "  out_list = []\n",
        "  for path in audio_dir_path.iterdir():\n",
        "    out_list += convert_path(path, target_suffix)\n",
        "  return out_list\n",
        "\n",
        "def convert_to_wavs(audio_path):\n",
        "  flac_files = []\n",
        "  for path in audio_path.iterdir():\n",
        "    flac_files += convert_path(path, '.flac')\n",
        "  for flac_path in tqdm(flac_files, 'flac -> wav'):\n",
        "    wav_path = flac_path.parent / f'{flac_path.stem}.wav'\n",
        "    flac_audio = AudioSegment.from_file(flac_path, \"flac\")\n",
        "    flac_audio.export(wav_path, format=\"wav\")\n",
        "    os.remove(flac_path)\n",
        "    \n",
        "def change_samplerate(audio_path, target_sr):\n",
        "  wav_files = []\n",
        "  for path in audio_path.iterdir():\n",
        "    wav_files += convert_path(path, '.wav')\n",
        "  for wav_path in tqdm(wav_files, 'change samplerate'):\n",
        "    wav_data, sr = torchaudio.load(wav_path)\n",
        "    wav_data = torchaudio.functional.resample(wav_data, orig_freq=sr, new_freq=target_sr)\n",
        "    torchaudio.save(wav_path, wav_data, target_sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wNJ5pJxtX1Ls"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "# remove other audio and files\n",
        "\n",
        "def remove_other(dataset_path):\n",
        "  for path in dataset_path.iterdir():\n",
        "    if path.suffix == '.json' and 'other' in path.name:\n",
        "      shutil.rmtree(path)\n",
        "\n",
        "  audio_path = dataset_path / 'audio'\n",
        "  for path in audio_path.iterdir():\n",
        "    if 'other' in path.name:\n",
        "      if path.is_dir():\n",
        "        shutil.rmtree(path)\n",
        "      else:\n",
        "        os.remove(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "a4BAtdmyjzBX"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# creates meta.csv and fills it with data of the dataset\n",
        "def create_meta_txt(dataset_path, out_txt_folder, speakers):\n",
        "  out_txt_folder = Path(out_txt_folder)\n",
        "  \n",
        "  for speaker_id in speakers:\n",
        "    speakers[speaker_id]['json_paths'] = []\n",
        "  for path in dataset_path.iterdir():\n",
        "    if path.suffix != '.json':\n",
        "      continue\n",
        "    assigned_speaker_id = None\n",
        "    for speaker_id in speakers:\n",
        "      if speaker_id in path.name:\n",
        "        assigned_speaker_id = speaker_id\n",
        "        break\n",
        "    if assigned_speaker_id is None:\n",
        "      print(f'Assigned speaker was not found: {path.name}')\n",
        "      continue\n",
        "    speakers[assigned_speaker_id]['json_paths'].append(path)\n",
        "  \n",
        "  for speaker_data in speakers.values():\n",
        "    speaker_items = []\n",
        "    for json_path in speaker_data['json_paths']:\n",
        "      with open(json_path, 'r') as json_file:\n",
        "        for row in json_file:\n",
        "          json_data = json.loads(row)\n",
        "          audio_path = Path(json_data['audio_filepath'])\n",
        "          file_path = audio_path.parent / f'{audio_path.stem}.wav'\n",
        "          global_file_path = dataset_path / file_path\n",
        "          text = json_data['text_normalized']\n",
        "          if global_file_path.exists():\n",
        "            speaker_items.append((file_path, text, speaker_data['our_idx']))\n",
        "          else:\n",
        "            print(f'File not found for {file_path}')\n",
        "    speaker_txt_path = out_txt_folder / f'{speaker_data[\"name\"]}_audio_text.txt'\n",
        "    with open(speaker_txt_path, 'w') as speaker_file:\n",
        "      for wav, text, speaker_our_idx in speaker_items:\n",
        "        speaker_file.write(f'{str(wav)}|{text}|{speaker_our_idx}\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7euwFVrUVkIh"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "readers = {\n",
        "    '9017': {'name': 'John_Van_Stan', 'our_idx': 1},\n",
        "    '6097': {'name': 'Phil_Benson', 'our_idx': 2},\n",
        "    '92': {'name': 'Cori_Samuel', 'our_idx': 3}\n",
        "}\n",
        "\n",
        "datasets_folder = Path('~/datasets').expanduser()\n",
        "dataset_path = datasets_folder / 'hi_fi_tts_v0'\n",
        "\n",
        "audio_path = dataset_path / 'audio'\n",
        "transcript_path = dataset_path\n",
        "out_txt_folder = Path('filelists')\n",
        "# wavs_path = dataset_path / 'wavs'\n",
        "# wavs_path.mkdir(exist_ok=True)\n",
        "\n",
        "# remove_other(dataset_path)\n",
        "# convert_to_wavs(audio_path)\n",
        "# create_meta_txt(dataset_path, out_txt_folder, readers)\n",
        "# change_samplerate(audio_path, 22050)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "/home/server2/datasets/hi_fi_tts_v0/hi_fi_tts_v0/audio/9017_clean/13885/dartagnan03part1_025_dumas_0071.wav",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m parts[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(wav_path)\n\u001b[1;32m     16\u001b[0m pitch_path \u001b[39m=\u001b[39m Path(\u001b[39m'\u001b[39m\u001b[39mhi_fi_tts_v0\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m/\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpitch\u001b[39m\u001b[39m'\u001b[39m \u001b[39m/\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mwav_path\u001b[39m.\u001b[39mstem\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 18\u001b[0m \u001b[39massert\u001b[39;00m (datasets_folder \u001b[39m/\u001b[39m wav_path)\u001b[39m.\u001b[39mexists(), \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdatasets_folder\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39mwav_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[39massert\u001b[39;00m (datasets_folder \u001b[39m/\u001b[39m pitch_path)\u001b[39m.\u001b[39mexists(), \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdatasets_folder\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39mpitch_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m     21\u001b[0m parts\u001b[39m.\u001b[39minsert(\u001b[39m1\u001b[39m, \u001b[39mstr\u001b[39m(pitch_path))\n",
            "\u001b[0;31mAssertionError\u001b[0m: /home/server2/datasets/hi_fi_tts_v0/hi_fi_tts_v0/audio/9017_clean/13885/dartagnan03part1_025_dumas_0071.wav"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "for _, speaker_data in readers.items():\n",
        "    list_file_path = out_txt_folder / f'{speaker_data[\"name\"]}_audio_text.txt'\n",
        "    out_lines = []\n",
        "    with open(list_file_path, 'r') as list_file:\n",
        "        lines = list_file.readlines()\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if len(line) == 0:\n",
        "                continue\n",
        "            parts = line.split('|')\n",
        "            wav_path = Path('hi_fi_tts_v0') / parts[0]\n",
        "            parts[0] = str(wav_path)\n",
        "            pitch_path = Path('hi_fi_tts_v0') / 'pitch' / f'{wav_path.stem}.pt'\n",
        "            \n",
        "            assert (datasets_folder / wav_path).exists(), f'{datasets_folder / wav_path}'\n",
        "            assert (datasets_folder / pitch_path).exists(), f'{datasets_folder / pitch_path}'\n",
        "            \n",
        "            parts.insert(1, str(pitch_path))\n",
        "            out_lines.append('|'.join(parts) + '\\n')\n",
        "    list_file_path = out_txt_folder / f'{speaker_data[\"name\"]}_audio_text.txt'\n",
        "    with open(list_file_path, 'w') as list_file:\n",
        "        list_file.writelines(out_lines)\n",
        "    np.random.shuffle(out_lines)\n",
        "    val_set = out_lines[:100]\n",
        "    train_set = out_lines[100:]\n",
        "    list_file_path = out_txt_folder / f'{speaker_data[\"name\"]}_audio_pitch_text_train.txt'\n",
        "    with open(list_file_path, 'w') as list_file:\n",
        "        list_file.writelines(train_set)\n",
        "    list_file_path = out_txt_folder / f'{speaker_data[\"name\"]}_audio_pitch_text_val.txt'\n",
        "    with open(list_file_path, 'w') as list_file:\n",
        "        list_file.writelines(val_set)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = []\n",
        "val_data = []\n",
        "\n",
        "with open('filelists/ljs_audio_pitch_text_train_v3.txt', 'r') as list_file:\n",
        "    lines = list_file.readlines()\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if len(line) == 0:\n",
        "            continue\n",
        "        parts = line.split('|')\n",
        "        wav_path = Path('LJSpeech-1.1') / parts[0]\n",
        "        parts[0] = str(wav_path)\n",
        "        pitch_path = Path('LJSpeech-1.1') / parts[1]\n",
        "        parts[1] = str(pitch_path)\n",
        "        \n",
        "        assert (datasets_folder / wav_path).exists(), f'{datasets_folder / wav_path}'\n",
        "        assert (datasets_folder / pitch_path).exists(), f'{datasets_folder / pitch_path}'\n",
        "            \n",
        "        train_data.append('|'.join(parts) + '\\n')\n",
        "\n",
        "with open('filelists/ljs_audio_pitch_text_val.txt', 'r') as list_file:\n",
        "    lines = list_file.readlines()\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if len(line) == 0:\n",
        "            continue\n",
        "        parts = line.split('|')\n",
        "        wav_path = Path('LJSpeech-1.1') / parts[0]\n",
        "        parts[0] = str(wav_path)\n",
        "        pitch_path = Path('LJSpeech-1.1') / parts[1]\n",
        "        parts[1] = str(pitch_path)\n",
        "        \n",
        "        assert (datasets_folder / wav_path).exists(), f'{datasets_folder / wav_path}'\n",
        "        assert (datasets_folder / pitch_path).exists(), f'{datasets_folder / pitch_path}'\n",
        "            \n",
        "        val_data.append('|'.join(parts) + '\\n')\n",
        "        \n",
        "train_list = []\n",
        "val_list = []\n",
        "for _, speaker_data in readers.items():\n",
        "    train_list_file_path = out_txt_folder / f'{speaker_data[\"name\"]}_audio_pitch_text_train.txt'\n",
        "    val_list_file_path = out_txt_folder / f'{speaker_data[\"name\"]}_audio_pitch_text_val.txt'\n",
        "    train_list.append(train_list_file_path)\n",
        "    val_list.append(val_list_file_path)\n",
        "    \n",
        "for train_list_file_path in train_list:\n",
        "    with open(train_list_file_path, 'r') as train_list_file:\n",
        "        lines = train_list_file.readlines()\n",
        "        train_data += lines\n",
        "with open(out_txt_folder / 'train.txt', 'w') as train_list_file:\n",
        "    train_list_file.writelines(train_data)\n",
        "    \n",
        "for val_list_file_path in val_list:\n",
        "    with open(val_list_file_path, 'r') as val_list_file:\n",
        "        lines = val_list_file.readlines()\n",
        "        val_data += lines\n",
        "with open(out_txt_folder / 'val.txt', 'w') as val_list_file:\n",
        "    val_list_file.writelines(val_data)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
